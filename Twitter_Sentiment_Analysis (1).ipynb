{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Gu5sNQpqnf"
      },
      "source": [
        "#1.importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xnfm8QTQZ1st"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C8D7h83rmC1",
        "outputId": "b472a2b2-5ca6-4eed-a4b8-7f830a5ad6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yb9mZwQrvEc",
        "outputId": "e7dd838f-6f14-4b29-e58d-ffa5a49b8bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ],
      "source": [
        "#printing the stopwords in english language\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBu-Cih8rjWE"
      },
      "source": [
        "#2.preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.framing the data into data frame"
      ],
      "metadata": {
        "id": "zQwbqqmRbN4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vxVdttShqv1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "ecfe824a-8cff-4d5b-a58e-8f84998e66a2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 39308",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9bba7cb24454>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading the data from csv file to pandas data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtwitter_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/twitter_training.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 39308"
          ]
        }
      ],
      "source": [
        "#loading the data from csv file to pandas data frame\n",
        "twitter_data = pd.read_csv('/content/twitter_training.csv',encoding= 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Slvwdq7reJ4"
      },
      "outputs": [],
      "source": [
        "#printing the no of rows and columns\n",
        "twitter_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDBWVxiNsh5A"
      },
      "outputs": [],
      "source": [
        "#printing the first five rows\n",
        "twitter_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.data managing"
      ],
      "metadata": {
        "id": "4rWmQp-dbU1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRg1X97It-Y2"
      },
      "outputs": [],
      "source": [
        "#naming the columns\n",
        "column_names=['id','flag','target','text']\n",
        "twitter_data = pd.read_csv('/content/twitter_training.csv', names = column_names, encoding= 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yxHeSGfdGw-"
      },
      "outputs": [],
      "source": [
        "twitter_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-R-9iCguvNy"
      },
      "outputs": [],
      "source": [
        "twitter_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjWWMem4vHaD"
      },
      "outputs": [],
      "source": [
        "#counting the no of missing values\n",
        "twitter_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data=twitter_data.dropna()\n",
        "twitter_data.isna().sum()"
      ],
      "metadata": {
        "id": "zR5lJjVwWDMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnKijbd4wFf5"
      },
      "outputs": [],
      "source": [
        "#checking the distribution of target column\n",
        "twitter_data['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.stemming\n"
      ],
      "metadata": {
        "id": "s7YatsobU1Sj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UHnBaWFyHnE"
      },
      "outputs": [],
      "source": [
        "port_stem = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9F5WQL2yXm2"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_tokenize(content):\n",
        "    # Removing non-alphabet characters and converting to lowercase\n",
        "    content = re.sub('[^a-zA-Z]', ' ', content).lower()\n",
        "    # Splitting and removing stopwords\n",
        "    tokens = [port_stem.stem(word) for word in content.split() if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the tweets\n",
        "twitter_data['tokens'] = twitter_data['text'].apply(preprocess_and_tokenize)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Assuming your target column is named 'target'\n",
        "label_encoder = LabelEncoder()\n",
        "twitter_data['target'] = label_encoder.fit_transform(twitter_data['target'])\n",
        "\n",
        "# Check the unique values after encoding\n",
        "print(\"Encoded classes:\", label_encoder.classes_)"
      ],
      "metadata": {
        "id": "Qa043AE1g4oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the data and labels\n",
        "x = twitter_data['tokens'].values\n",
        "y = twitter_data['target'].values"
      ],
      "metadata": {
        "id": "vltQmzwrg0CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKx_sKzzzqkH"
      },
      "outputs": [],
      "source": [
        "twitter_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8PVxq_v3zf2"
      },
      "outputs": [],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxLYTqLs32om"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxX0u1rd3yUS"
      },
      "source": [
        "#3.Splitting the data into training data and test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIHq9Frg4Ki_"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1mxTKVY4m25"
      },
      "outputs": [],
      "source": [
        "print(x.shape,x_train.shape,x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.vectorization\n"
      ],
      "metadata": {
        "id": "7OBtsBo6bo-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "eiha1k7kjCH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Word2Vec Embeddings\n",
        "# Training Word2Vec model on the training data\n",
        "w2v_model = Word2Vec(sentences=x_train.tolist(), vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Function to get the average word vector for each tweet\n",
        "def get_average_word2vec(tokens, model, vector_size):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "# Creating average word vectors for each tweet in training and test sets\n",
        "x_train_vec = np.array([get_average_word2vec(tokens, w2v_model, 100) for tokens in x_train])\n",
        "x_test_vec = np.array([get_average_word2vec(tokens, w2v_model, 100) for tokens in x_test])"
      ],
      "metadata": {
        "id": "1uo8bB32kzi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJVIkLUN6AUx"
      },
      "source": [
        "# 4.Training the Machine learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2my_XgXN6Hci"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmp7q1MH5ndU"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(x_train_vec,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6q7MaJb6ti_"
      },
      "outputs": [],
      "source": [
        "x_train_prediction = lr.predict(x_train_vec)\n",
        "training_data_accuracy = accuracy_score(x_train_prediction,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHgfiI2d6yDV"
      },
      "outputs": [],
      "source": [
        "print('Accuracy score of the training data: ',training_data_accuracy*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "clf_xgb=xgb.XGBClassifier()\n",
        "clf_xgb.fit(x_train_vec,y_train)\n",
        "x_train_prediction_xgb=clf_xgb.predict(x_train_vec)\n",
        "training_data_accuracy_xgb=accuracy_score(x_train_prediction_xgb,y_train)\n",
        "print(training_data_accuracy_xgb*100)"
      ],
      "metadata": {
        "id": "GiD4V9V9cPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#random forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(x_train_vec, y_train)"
      ],
      "metadata": {
        "id": "A4lTkIlLxUhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_prediction_rf = rf_model.predict(x_train_vec)\n",
        "accuracy_rf = accuracy_score(y_train, x_train_prediction_rf)\n",
        "#print(f'Accuracy of Random Forest model: {accuracy_rf:.2f}')\n",
        "print(accuracy_rf*100)"
      ],
      "metadata": {
        "id": "2KH7z1OtyV6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming x_train_vec is your feature matrix with potential negative values\n",
        "scaler = MinMaxScaler()\n",
        "x_train_vec_scaled = scaler.fit_transform(x_train_vec)\n",
        "x_test_vec_scaled = scaler.transform(x_test_vec)"
      ],
      "metadata": {
        "id": "Cb0OWBluRjXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23miLpHX7ZLd"
      },
      "source": [
        "MN Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK1DkaSb7gcn"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Now fit the MultinomialNB model\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(x_train_vec_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojI8Irma7m20"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train_prediction_nb = classifier.predict(x_train_vec_scaled)\n",
        "training_datas_accuracy_nb = accuracy_score(x_train_prediction_nb,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy score of the training data: ',training_datas_accuracy_nb*100)"
      ],
      "metadata": {
        "id": "tVw6Pu1hYWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('logistic_regression', lr),\n",
        "    ('xgboost', clf_xgb),\n",
        "    ('naive_bayes', classifier),\n",
        "    ('randomforest', rf_model)\n",
        "], voting='hard')  # Use 'soft' for probability-based voting\n",
        "\n",
        "# Train the ensemble model on the training data\n",
        "ensemble_model.fit(x_train_vec_scaled, y_train)\n",
        "\n",
        "# Evaluate the ensemble model on training data\n",
        "y_train_pred_ensemble = ensemble_model.predict(x_train_vec_scaled)\n",
        "training_accuracy_ensemble = accuracy_score(y_train_pred_ensemble, y_train)\n",
        "print('Ensemble Model Accuracy on Training Data: ', training_accuracy_ensemble * 100)"
      ],
      "metadata": {
        "id": "41oUCKRenFDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training accuracies\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['Logistic Regression', 'XGBoost', 'Naive Bayes', 'RandomForest']\n",
        "accuracies = [\n",
        "    training_data_accuracy * 100,\n",
        "    training_data_accuracy_xgb * 100,\n",
        "    training_datas_accuracy_nb * 100,\n",
        "    accuracy_rf * 100\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=models, y=accuracies)\n",
        "plt.title('Training Accuracy of Different Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.ylim(0, 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hi558ZyR5-k0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}